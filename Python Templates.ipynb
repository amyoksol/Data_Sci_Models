{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load basic packages\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "import time\n",
    "from operator import itemgetter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-no missing values\n",
    "\n",
    "-make target variable binary and move to column position 0\n",
    "\n",
    "-transform any categorical/factor variables\n",
    "\n",
    "-delete any variables that do not have predictive power (i.e. ID)\n",
    "\n",
    "-check for skewness of any variables\n",
    "\n",
    "-check for balance with target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test/Train Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Choose size of test/train datasets by changing \"test_size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(df.iloc[:,1:].values, df.iloc[:,0].values, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (K Nearest Neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Lazy learner,\" looks at k nearest neighbors to determine where to classify a datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#build model\n",
    "knn = KNeighborsClassifier()\n",
    "knn = knn.fit(features_train, target_train) #Fit to training data\n",
    "target_predicted_knn = knn.predict(features_test) #Predict model again test data\n",
    "\n",
    "#check accuracy\n",
    "print(\"Knn Accuracy Score\", accuracy_score(target_test, target_predicted_knn))\n",
    "print(classification_report(target_test, target_predicted_knn)) \n",
    "cm=confusion_matrix(target_test, target_predicted_knn)\n",
    "print(cm2)\n",
    "plt.matshow(cm2)\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "#cross validate \n",
    "scores = cross_val_score(knn, features_train, target_train, cv=10)\n",
    "print(\"Cross Validation Score for each K\",scores)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Greedy algorithm,\" variables are split into buckets based on how much they contribute to the decision, attribute with highest information gain is first place to split, then repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree \n",
    "\n",
    "#build model\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "dt = dt.fit(features_train, target_train)\n",
    "target_predicted_dt = dt.predict(features_test)\n",
    "\n",
    "#check accuracy\n",
    "print(\"DT Accuracy Score\", accuracy_score(target_test, target_predicted_dt))\n",
    "print(classification_report(target_test, target_predicted_dt))\n",
    "#confusion matrix\n",
    "cm=confusion_matrix(target_test, target_predicted_dt)\n",
    "print(cm)\n",
    "plt.matshow(cm)\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "#cross validate\n",
    "scores = cross_val_score(dt, features_train, target_train, cv=10)\n",
    "print(\"Cross Validation Score for each K\",scores)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm uses a random number of \"extra\" decision trees on subsets of data; then aggregates using an average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "#build model\n",
    "xdt = ExtraTreesClassifier()\n",
    "xdt.fit(features_train, target_train)\n",
    "predicted_xdt=xdt.predict(features_test)\n",
    "expected = target_test\n",
    "\n",
    "#checking accuracy\n",
    "print(\"Extra Trees\", accuracy_score(expected,predicted_xdt))\n",
    "print(classification_report(expected, predicted_xdt))\n",
    "cm=confusion_matrix(expected, predicted_xdt)\n",
    "print(cm)\n",
    "plt.matshow(cm)\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "#cross validate\n",
    "scores = cross_val_score(xdt, features_train, target_train, cv=10)\n",
    "print(\"Cross Validation Score for each K\",scores)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average (or mode) of many decision trees, often less overfitting than decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#build model\n",
    "rf = RandomForestClassifier()\n",
    "rf = rf.fit(features_train, target_train)\n",
    "target_predicted_rf = rf.predict(features_test)\n",
    "\n",
    "#check accuracy\n",
    "print(\"RF Accuracy Score\", accuracy_score(target_test, target_predicted_rf))\n",
    "print(classification_report(target_test, target_predicted_rf))\n",
    "#confusion matrix\n",
    "cm=confusion_matrix(target_test, target_predicted_rf)\n",
    "print(cm)\n",
    "plt.matshow(cm)\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "#cross validate\n",
    "scores = cross_val_score(rf, features_train, target_train, cv=10)\n",
    "print(\"Cross Validation Score for each K\",scores)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging resamples training data, performs bootstrapping, and then aggregates (either by voting or an average); this method improves on the performance of weak learners (in this example I improve upon a knn model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#build model\n",
    "clf_bag = BaggingClassifier(KNeighborsClassifier())\n",
    "clf_bag.fit(features_train, target_train)\n",
    "predicted_bag=clf_bag.predict(features_test)\n",
    "expected = target_test\n",
    "\n",
    "#check accuracy\n",
    "print(\"Bagging Accuracy\", accuracy_score(expected,predicted_bag))\n",
    "print(classification_report(expected, predicted_bag))\n",
    "cm=confusion_matrix(expected, predicted_bag)\n",
    "print(cm)\n",
    "plt.matshow(cm)\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "#cross validate\n",
    "scores = cross_val_score(clf_bag, features_train, target_train, cv=10)\n",
    "print(\"Cross Validation Score for each K\",scores)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm can also be used to boost performance of weak learners (this example boosts a typical decision tree); combines output of weaker learners using a weighted sum; uses an iterative procedure so that new models are influenced by previously built ones (the weighting allows there to be a focus on what previous models got wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#build model\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier())\n",
    "bdt.fit(features_train, target_train)\n",
    "predicted_bdt=bdt.predict(features_test)\n",
    "expected = target_test\n",
    "\n",
    "#check accuracy\n",
    "print(\"Adaboost Accuracy\", accuracy_score(expected,predicted_bdt))\n",
    "print(classification_report(expected, predicted_bdt))\n",
    "cm=confusion_matrix(target_test, predicted_bdt)\n",
    "print(cm)\n",
    "plt.matshow(cm)\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "#cross validate\n",
    "scores = cross_val_score(bdt, features_train, target_train, cv=3)\n",
    "print(\"Cross Validation Score for each K\",scores)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds the minimum value of cuntions where a closed form solution is not easily obtainable or available; forms partial differentiation equations, sets to 0, and solves; we want to find the minimum because that's where the lowest error is (and the best model); the \"gradient\" refers to the function you are trying to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#build model\n",
    "sgd = SGDClassifier() \n",
    "sgd.fit(features_train, target_train)\n",
    "predicted_sgd=sgd.predict(features_test)\n",
    "expected = target_test\n",
    "\n",
    "#check accuracy\n",
    "print(\"SGD Accuracy Score\", accuracy_score(expected,predicted_sgd))\n",
    "print(classification_report(expected, predicted_sgd))\n",
    "cm=confusion_matrix(target_test, predicted_sgd)\n",
    "print(cm)\n",
    "plt.matshow(cm)\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "#cross validate\n",
    "scores = cross_val_score(sgd, features_train, target_train, cv=10)\n",
    "print(\"Cross Validation Score for each K\",scores)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combination of gradient descent and boosting; uses weak learners to progressively improve upon shortcomings of previous weak learners "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#build model\n",
    "GBC = GradientBoostingClassifier()\n",
    "GBC.fit(features_train, target_train)\n",
    "predicted_GBC=GBC.predict(features_test)\n",
    "expected = target_test\n",
    "\n",
    "#checking accuracy\n",
    "print(\"Gradient Boost Accuracy\", accuracy_score(expected,predicted_GBC))\n",
    "print(classification_report(expected, predicted_GBC))\n",
    "cm=confusion_matrix(expected, predicted_GBC)\n",
    "print(cm)\n",
    "plt.matshow(cm)\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "#cross validate\n",
    "scores = cross_val_score(GBC, features_train, target_train, cv=10)\n",
    "print(\"Cross Validation Score for each K\",scores)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SVM Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Support Vector Machine model; searches for linear optimal separating hyperplane (decision boundary) using support vectors and margins; the support vectors define the distance and direction of the hyperplane while the margin gives the distance between support vectors; a kernel chooses a hyperplane that gives the largest margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#build model\n",
    "clf_linSVC=LinearSVC()\n",
    "clf_linSVC.fit(features_train, target_train)\n",
    "predicted_SVC=clf_linSVC.predict(features_test)\n",
    "expected = target_test\n",
    "\n",
    "#checking accuracy\n",
    "print(accuracy_score(expected,predicted_SVC))\n",
    "print(classification_report(expected, predicted_SVC))\n",
    "cm=confusion_matrix(expected, predicted_SVC)\n",
    "print(cm)\n",
    "plt.matshow(cm)\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "#cross validate\n",
    "scores = cross_val_score(clf_linSVC, features_train, target_train, cv=10)\n",
    "print(\"Cross Validation Score for each K\",scores)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine using Radial Basis Function kernel; \"kernel trick\" = mapping original feature space to some higher-dimensional feature space where the training set is seperable\n",
    "\n",
    "*may take awhile to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#build model\n",
    "clf_rbf = SVC(kernel='rbf')\n",
    "clf_rbf.fit(features_train, target_train)\n",
    "predicted_rbf=clf_rbf.predict(features_test)\n",
    "expected = target_test\n",
    "\n",
    "#checking accuracy\n",
    "print(accuracy_score(expected,predicted_rbf))\n",
    "print(classification_report(expected, predicted_rbf))\n",
    "cm=confusion_matrix(expected, predicted_rbf)\n",
    "print(cm)\n",
    "plt.matshow(cm)\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "#cross validate\n",
    "scores = cross_val_score(clf_rbf, features_train, target_train, cv=10)\n",
    "print(\"Cross Validation Score for each K\",scores)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Network, set of connected input/output units where each connection has a weight associated with it; during learning phase, network readjusts the weights so it can better predict the correct class label; similar to way the brain learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#build model\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(features_train, target_train)\n",
    "predicted_mlp=mlp.predict(features_test)\n",
    "expected = target_test\n",
    "\n",
    "#checking accuracy\n",
    "print(accuracy_score(expected,predicted_mlp))\n",
    "print(classification_report(expected, predicted_mlp))\n",
    "cm=confusion_matrix(expected, predicted_mlp)\n",
    "print(cm)\n",
    "plt.matshow(cm)\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "#cross validate\n",
    "scores = cross_val_score(mlp, features_train, target_train, cv=10)\n",
    "print(\"Cross Validation Score for each K\",scores)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Stacks\" several different predictive models ontop of each other to form a new model; often has better performance than the individual models alone; this example stacks Random Forest, Gaussian Naive Bayes, and Adaboost Classifiers with \"hard\" voting (meaning the final class label is predicted by whatever class has been predicted most frequently by the classification models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#build model\n",
    "clf1 = RandomForestClassifier()\n",
    "clf2 = GaussianNB()\n",
    "bdt = AdaBoostClassifier()\n",
    "eclf2 = VotingClassifier(estimators=[('rf', clf1), ('gnb', clf2), ('bdt', bdt)], voting='hard')\n",
    "eclf2.fit(features_train, target_train)\n",
    "predicted_eclf2=eclf2.predict(features_test)\n",
    "expected = target_test\n",
    "\n",
    "#checking accuracy\n",
    "print(accuracy_score(expected,predicted_eclf2))\n",
    "print(classification_report(expected, predicted_eclf2))\n",
    "cm=confusion_matrix(expected, predicted_eclf2)\n",
    "print(cm)\n",
    "plt.matshow(cm)\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "#cross validate\n",
    "scores = cross_val_score(eclf2, features_train, target_train, cv=10)\n",
    "print(\"Cross Validation Score for each K\",scores)\n",
    "scores.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
